{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMIFQdcO7sPR7QlD6lu/NDH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# _Kaggle Competition Classification"],"metadata":{"id":"ynlJ40nUK1Ot"}},{"cell_type":"markdown","source":["## Importing Basic Libraries"],"metadata":{"id":"C16-HGs-KaGm"}},{"cell_type":"code","source":["from sklearn.metrics import (confusion_matrix, accuracy_score,\n","                             precision_score, f1_score, recall_score,\n","                             roc_auc_score, roc_curve, auc\n","                             )\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.dummy import DummyClassifier\n","\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.neighbors import VALID_METRICS\n","\n","from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n","from sklearn.ensemble import (AdaBoostClassifier, ExtraTreesClassifier,\n","                              BaggingClassifier, RandomForestClassifier,\n","                              GradientBoostingClassifier,\n","                              HistGradientBoostingClassifier,\n","                              StackingClassifier\n","                              )\n","from lightgbm import LGBMClassifier\n","from xgboost import XGBClassifier, XGBRFClassifier\n","from catboost import CatBoostClassifier"],"metadata":{"id":"mqfkS-dDh_Xt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Testing algorithms"],"metadata":{"id":"--bJ_xjnJ2go"}},{"cell_type":"code","source":["def select_model(dataframe, test_size=.3, target_feature=target_feature, model = None):\n","    !mkdir -p output/data\n","    algos = [\n","            #  SVC(), NuSVC(),\n","            #  LinearRegression(),\n","            DecisionTreeClassifier(), ExtraTreeClassifier(),\n","            AdaBoostClassifier(), BaggingClassifier(),\n","            ExtraTreesClassifier(), RandomForestClassifier(),\n","            GradientBoostingClassifier(),\n","            HistGradientBoostingClassifier(),\n","            LGBMClassifier(), XGBClassifier(), XGBRFClassifier(),\n","            CatBoostClassifier()\n","            ]\n","    names = list(map(lambda x: x.__class__.__name__, algos))\n","\n","    history = { 'algo_name': names,\n","                'train_score' : [],\n","                'test_score' : [],\n","                'train_metrics': [],\n","                'test_metrics': [],\n","                }\n","    X_train, X_test, y_train, y_test = train_test_split(dataframe.drop(target_feature, axis=1), dataframe[target_feature], test_size=test_size)\n","    for model in algos:\n","        print(f\"{model.__class__.__name__} has started!\")\n","        model.fit(X_train, y_train)\n","        pred_train = model.predict(X_train)\n","        pred_test = model.predict(X_test)\n","        history['train_score'].append(accuracy_score(y_train, pred_train))\n","        history['test_score'].append(accuracy_score(y_test, pred_test))\n","        try:\n","            y_prob = model.predict_proba(X_train)[:, 1]\n","            roc_auc = roc_auc_score(y_train, y_prob)\n","            history['train_metrics'].append(roc_auc)\n","            y_prob_ = model.predict_proba(X_test)[:, 1]\n","            roc_auc_ = roc_auc_score(y_test, y_prob_)\n","            history['test_metrics'].append(roc_auc_)\n","        except Exception as e:\n","            history['train_metrics'].append(np.nan)\n","            history['test_metrics'].append(np.nan)\n","    return pd.DataFrame(history)"],"metadata":{"id":"e4kmfj6yzjSQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_score = lambda model: (model.score(X_train, y_train), model.score(X_test, y_test))"],"metadata":{"id":"BYC-m9fQogzQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# optuna objective function for hyperparameter tuning of LGBM\n","def objective(trial):\n","    params = {\n","        'boosting_type'    : trial.suggest_categorical('boosting_type', ['gbdt', 'dart']),\n","        'num_leaves'        : trial.suggest_int('num_leaves', 10, 100),\n","        'max_depth'         : trial.suggest_categorical('max_depth', [-1,3, 4, 5]),\n","        'learning_rate'     : trial.suggest_float('learning_rate', .001, 2, log=True),\n","        # 'n_estimators'      : trial.suggest_int('n_estimators', 100, 1000),\n","\n","\n","        'min_split_gain'    : trial.suggest_float('min_split_gain', 0, 1),\n","        'min_child_weight'  : trial.suggest_float('min_child_weight', 0, 1),\n","        'min_child_samples' : trial.suggest_int('min_child_samples', 20, 100),\n","\n","        'subsample'         : trial.suggest_categorical('subsample', [.4, .5, .67, .75, .8, .9, .95, 1.0]),\n","        'colsample_bytree'  : trial.suggest_categorical('colsample_bytree', [.4, .5, .67, .75, .8, .9, .95, 1.0]),\n","\n","        'reg_alpha'         : trial.suggest_float('reg_alpha', 0, 1),\n","        'reg_lambda'        : trial.suggest_float('reg_lambda', 0, 1),\n","        }\n","\n","    model = LGBMClassifier(**params, verbose=-1,random_state=6547)\n","    model.fit(X_train, y_train)\n","    train_score, test_score = get_score(model)\n","    return test_score, test_score-train_score\n","\n","\n","study_name = 'LGBM_study'\n","storage = optuna.storages.RDBStorage(f\"sqlite:///output/logs/{study_name}.db\")\n","study = optuna.create_study(directions= ['maximize', 'maximize' ],storage = storage, study_name = study_name, load_if_exists=True)\n","study.optimize(objective, n_trials = 200)"],"metadata":{"id":"_dvR4bmboY9o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def objective(trial):\n","\n","    params = {\n","        'max_depth'         : trial.suggest_categorical('max_depth', [3, 4, 5, 6, None]),\n","        'max_leaves'        : trial.suggest_int('max_leaves', 0,100),\n","        'grow_policy'       : trial.suggest_categorical('grow_policy', ['depthwise','lossguide', None]),\n","        'learning_rate'     : trial.suggest_float('learning_rate', .001, 2, log=True),\n","        'tree_method'       : trial.suggest_categorical('tree_method', ['exact', 'approx', 'hist', None]),\n","        'gamma'             : trial.suggest_categorical('gamma',[0, 1e-4, 1e-3, 1e-1, None] ),\n","        'subsample'         : trial.suggest_categorical('subsample',[ .4, .5, .67, .75, .8, .9, 1.0] ),\n","        'colsample_bytree'  : trial.suggest_categorical('colsample_bytree',[ .4, .5, .67, .75, .8, .9, 1.0] ),\n","        'colsample_bylevel' : trial.suggest_categorical('colsample_bylevel',[ .4, .5, .67, .75, .8, .9, 1.0] ),\n","        'colsample_bynode'  : trial.suggest_categorical('colsample_bynode',[ .4, .5, .67, .75, .8, .9, 1.0] ),\n","        'reg_alpha'         : trial.suggest_float('reg_alpha', 0, 1),\n","        'reg_lambda'        : trial.suggest_float('reg_lambda', 0, 1),\n","        'importance_type'   : trial.suggest_categorical('importance_type', ['gain', 'weight', 'cover', 'total_gain', 'total_cover', None]),\n","    }\n","\n","    model = XGBClassifier(**params)\n","\n","    try:\n","        model.fit(X_train, y_train)\n","        train_score, test_score = get_score(model)\n","        return test_score, test_score-train_score\n","    except Exception as e:\n","        return -np.inf, -np.inf\n"],"metadata":{"id":"DzRBpZbgoyCa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def objective(trial):\n","    params = {\n","        'learning_rate'                 : trial.suggest_float('learning_rate', .001, 2, log=True),\n","        'objective'                     : trial.suggest_categorical('objective', ['Logloss', 'CrossEntropy']),\n","        'depth'                         : trial.suggest_int('depth', 3, 10),\n","        'reg_lambda'                    : trial.suggest_float('reg_lambda', 0, 10),\n","        'subsample'                     : trial.suggest_float('subsample', 0, 1),\n","        'colsample_bylevel'             : trial.suggest_float('colsample_bylevel', 0, 1),\n","        'min_child_samples'             : trial.suggest_int('min_child_samples', 10, 100),\n","        'leaf_estimation_iterations'    : trial.suggest_int('leaf_estimation_iterations', 5, 20),\n","\n","\n","    }\n","\n","    model = CatBoostClassifier(**params,iterations=100,verbose=0,random_state = 84987)\n","\n","    try:\n","        model.fit(X_train, y_train)\n","        train_score, test_score = get_score(model)\n","        return test_score, test_score-train_score\n","    except Exception as e:\n","        return -np.inf, -np.inf"],"metadata":{"id":"vWMWrshNo1EY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["params = [\n","    {'learning_rate': 0.19068478145864712, 'objective': 'Logloss', 'depth': 9, 'reg_lambda': 1.3927531091623635, 'subsample': 0.6651469314640843, 'colsample_bylevel': 0.9706601048429702, 'min_child_samples': 50, 'leaf_estimation_iterations': 12},\n","    {'learning_rate': 0.174982304220691, 'objective': 'Logloss', 'depth': 8, 'reg_lambda': 7.512299289102047, 'subsample': 0.4877580683039361, 'colsample_bylevel': 0.6151802353705157, 'min_child_samples': 70, 'leaf_estimation_iterations': 12},\n","    {'learning_rate': 0.19068478145864712, 'objective': 'Logloss', 'depth': 7, 'reg_lambda': 8.947085765030208, 'subsample': 0.3934334059883876, 'colsample_bylevel': 0.8078464837389985, 'min_child_samples': 94, 'leaf_estimation_iterations': 8},\n","    {'learning_rate': 0.05895050294121335, 'objective': 'Logloss', 'depth': 10, 'reg_lambda': 9.058283844063986, 'subsample': 0.08931663638769871, 'colsample_bylevel': 0.9694763077811444, 'min_child_samples': 26, 'leaf_estimation_iterations': 12},\n","    {'learning_rate': 0.06614189893121368, 'objective': 'Logloss', 'depth': 6, 'reg_lambda': 9.042807154267283, 'subsample': 0.5314518336396845, 'colsample_bylevel': 0.6119706476986938, 'min_child_samples': 22, 'leaf_estimation_iterations': 6}\n","]\n","\n","for i, p in enumerate(params):\n","    name = f'CatBoost_{i}'\n","    model = CatBoostClassifier(**p,iterations=100,verbose=0,random_state = 84987)\n","    model.fit(X_train,y_train)\n","    a, b = get_score(model)\n","    c = matthews_corrcoef(train['class'], model.predict(train.drop('class',axis=1)))\n","    experimental_study['name'].append(name)\n","    experimental_study['train_score'].append(a)\n","    experimental_study['test_score'].append(b)\n","    experimental_study['score'].append(c)\n","    experimental_study['params'].append(p)"],"metadata":{"id":"ekbKdrWro4-4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# _Kaggle Competition Regression"],"metadata":{"id":"YbhBEwYBNCpN"}},{"cell_type":"code","source":["from sklearn.metrics import (root_mean_squared_log_error as RMLE,\n","                             r2_score)\n","# 'neg_root_mean_squared_log_error'\n","from sklearn.model_selection import train_test_split\n","\n","from sklearn.linear_model import LinearRegression\n","\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.neighbors import VALID_METRICS\n","\n","from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n","from sklearn.ensemble import (AdaBoostRegressor, ExtraTreesRegressor,\n","                              BaggingRegressor, RandomForestRegressor,\n","                              GradientBoostingRegressor,\n","                              HistGradientBoostingRegressor,\n","                              StackingRegressor\n","                              )\n","from lightgbm import LGBMRegressor\n","from xgboost import XGBRegressor, XGBRFRegressor\n","from catboost import CatBoostRegressor"],"metadata":{"id":"LWKCpbGyDpu4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def select_model(dataframe, test_size=.3, target_feature=target_feature, model = None):\n","    !mkdir -p output/data\n","    algos = [\n","            #  SVC(), NuSVC(),\n","            #  LinearRegression(),\n","            DecisionTreeRegressor(), ExtraTreeRegressor(),\n","            AdaBoostRegressor(), BaggingRegressor(),\n","            ExtraTreesRegressor(), RandomForestRegressor(),\n","            GradientBoostingRegressor(),\n","            HistGradientBoostingRegressor(),\n","            LGBMRegressor(verbose=-1), XGBRegressor(), XGBRFRegressor(),\n","            CatBoostRegressor(verbose=0)\n","            ]\n","    names = list(map(lambda x: x.__class__.__name__, algos))\n","\n","    history = { 'algo_name': names,\n","                'train_score' : [],\n","                'test_score' : [],\n","                # 'Min_1': [],\n","                # 'Min_2': [],\n","                'train_metrics': [],\n","                'test_metrics': [],\n","                }\n","    X_train, X_test, y_train, y_test = train_test_split(dataframe.drop(target_feature, axis=1), dataframe[target_feature], test_size=test_size)\n","    for model in algos:\n","        print(f\"{model.__class__.__name__} has started!\")\n","        model.fit(X_train, y_train)\n","        pred_train = model.predict(X_train)\n","        pred_test = model.predict(X_test)\n","        history['train_score'].append(r2_score(y_train, pred_train))\n","        history['test_score'].append(r2_score(y_test, pred_test))\n","        try:\n","            # history['Min_1'].append(np.min(pred_train))\n","            # history['Min_2'].append(np.min(pred_test))\n","\n","            history['train_metrics'].append(RMLE(y_train, pred_train))\n","            history['test_metrics'].append(RMLE(y_test, pred_test))\n","        except Exception as e:\n","            # history['Min_1'].append(np.nan)\n","            # history['Min_2'].append(np.nan)\n","            history['train_metrics'].append(np.nan)\n","            history['test_metrics'].append(np.nan)\n","    clear_output()\n","    return pd.DataFrame(history)"],"metadata":{"id":"HL7pohGUJk_p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1topYocUoPCQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#_Common functions"],"metadata":{"id":"geoYqWlBYdP3"}},{"cell_type":"code","source":["# @title Kaggle Competition Data Downloading  { display-mode: \"form\",run :\"auto\" }\n","# @markdown Put the competition name\n","kaggle_username = 'Kaggle_Username'     # @param ['Kaggle_Username', 'kaggle_username', 'kaggle_2']\n","kaggle_token = \"Kaggle\"     # @param [\"Kaggle\", 'kaggle', 'kaggle_2_pass']\n","\n","project_name = 'playground-series-s4e5'  # @param {type: \"string\"}\n","\n","\n","from google.colab import userdata\n","from IPython import display\n","import os\n","token = {\"username\":userdata.get(kaggle_username),\"key\":userdata.get(kaggle_token)}\n","import os, json\n","os.environ['KAGGLE_CONFIG_DIR']='.'\n","with open('kaggle.json', \"w\") as f:\n","    json.dump(token, f)\n","!chmod 600 ./kaggle.json\n","!kaggle competitions download -c $project_name\n","filename = project_name + \".zip\"\n","!unzip $project_name && rm $filename\n","os.environ['MLFLOW_TRACKING_PASSWORD'] = userdata.get('MLFLOW_TRACKING_PASSWORD')\n","os.environ['MLFLOW_TRACKING_USERNAME'] = userdata.get('MLFLOW_TRACKING_USERNAME')\n","display.clear_output()\n","print(\"Files have been downloaded!\")"],"metadata":{"id":"OPj78NupJPN3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!kaggle datasets download brijlaldhankour/flood-prediction-factors && unzip *.zip && rm -d *.zip"],"metadata":{"id":"Tj9J-UckIzl4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install optuna-dashboard optuna\n","!pip install mlflow dagshub\n","!pip install catboost\n","!mkdir -p output/models  output/logs output/plots\n","!pip install --upgrade gdown\n","display.clear_output()\n","import dagshub\n","import mlflow\n","import optuna"],"metadata":{"id":"0F5V4Ku5exLC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy.stats import anderson,kstest, shapiro\n","from IPython.display import Audio,display as dis\n","from IPython.display import clear_output\n","\n","from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.linear_model import LinearRegression, Ridge"],"metadata":{"id":"xAlqAv-UNHwm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Classification\n","from sklearn.metrics import (confusion_matrix, accuracy_score,\n","                             precision_score, f1_score, recall_score,\n","                             roc_auc_score, roc_curve, auc\n","                             )\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.dummy import DummyClassifier\n","\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.neighbors import VALID_METRICS\n","\n","from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n","from sklearn.ensemble import (AdaBoostClassifier, ExtraTreesClassifier,\n","                              BaggingClassifier, RandomForestClassifier,\n","                              GradientBoostingClassifier,\n","                              HistGradientBoostingClassifier,\n","                              StackingClassifier\n","                              )\n","from lightgbm import LGBMClassifier\n","from xgboost import XGBClassifier, XGBRFClassifier\n","from catboost import CatBoostClassifier\n","\n","# Regression Models\n","from sklearn.metrics import (root_mean_squared_log_error as RMLE,\n","                             r2_score)\n","# 'neg_root_mean_squared_log_error'\n","from sklearn.model_selection import train_test_split\n","\n","from sklearn.linear_model import LinearRegression\n","\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.neighbors import VALID_METRICS\n","\n","from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n","from sklearn.ensemble import (AdaBoostRegressor, ExtraTreesRegressor,\n","                              BaggingRegressor, RandomForestRegressor,\n","                              GradientBoostingRegressor,\n","                              HistGradientBoostingRegressor,\n","                              StackingRegressor\n","                              )\n","from lightgbm import LGBMRegressor\n","from xgboost import XGBRegressor, XGBRFRegressor\n","from catboost import CatBoostRegressor"],"metadata":{"id":"eI7_xw7QlNRY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plotly libraries setup\n","import plotly.express as px\n","from plotly.subplots import make_subplots\n","import plotly.graph_objs as go\n","import plotly.io as pio\n","pio.renderers.default = \"colab\"\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_colwidth', None)\n","# import plotly.offline as pyo\n","# pyo.init_notebook_mode()"],"metadata":{"id":"souVy-hce3Qb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dagshub.init(\"Kaggle-Competitions-Lab\", \"SantanuK\", mlflow=True)\n","experiment_name = ''\n","try:\n","    mlflow.create_experiment(experiment_name)\n","    mlflow.set_experiment(experiment_name)\n","except Exception as e:\n","    mlflow.set_experiment(experiment_name)\n","    # print(\"Experiment has been created or some issue occured!\\n\",\"Error: \", e )"],"metadata":{"id":"QJePqT1ylshE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Xcjo3zWgfL79"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train = pd.read_csv('train.csv').drop('id',axis=1)\n","df_test = pd.read_csv('test.csv').drop('id',axis=1)\n","df_extra = pd.read_csv('original.csv')"],"metadata":{"id":"0SnJfQPUNnX4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"\"\"The number of features: {df_test.shape[1]}\n","-----------------\n","Training DataSet\n","-----------------\n","The number of samples:              {df_train.shape[0]}\n","The number of duplicated samples:   {df_train.duplicated().sum()}\n","The number of null samples:         {df_train.isna().sum().sum()} ({round(df_train.isna().sum().sum()/df_train.shape[0]*100,2)}%)\n","The number of unique samples:       {df_train.nunique().sum()}\n","\n","-----------------\n","Testing DataSet\n","-----------------\n","The number of samples:              {df_test.shape[0]}\n","The number of duplicated samples:   {df_test.duplicated().sum()}\n","The number of null samples:         {df_test.isna().sum().sum()}({round(df_test.isna().sum().sum()/df_test.shape[0]*100,2)}%)\n","The number of unique samples:       {df_test.nunique().sum()}\n","\n","\"\"\")"],"metadata":{"id":"MwVruew1ymI7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_features =[]\n","cat_features =[]\n","target_feature = 'FloodProbability'"],"metadata":{"id":"1ofT4fUzNl5p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wdG3NXmnx-et"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features = df_test.columns\n","\n","\n","def visualization(dataframe, features=features):\n","    summary = {\n","        'Columns'   :   [],\n","        'Count'     :   [],\n","        'Unique count': [],\n","        'Max'       :   [],\n","        'Min'       :   [],\n","        'Mean'      :   [],\n","        'Std'       :   [],\n","        '5%'        :   [],\n","        '25%'        :   [],\n","        '50%'        :   [],\n","        '75%'        :   [],\n","        '95%'        :   [],\n","        # IQR, Low Bound, High Bound\n","        'IQR'       :   [],\n","        'Low Bound' :   [],\n","        'High Bound':   [],\n","        '< Low Bound' :   [],\n","        '> High Bound':   [],\n","        'Outliers'  : [],\n","        '% Outliers'  : [],\n","    }\n","    for col in features:\n","        summary['Columns'].append(col)\n","        summary['Count'].append(dataframe[col].notnull().sum())\n","        summary['Unique count'].append(dataframe[col].nunique())\n","        summary['Max'].append(dataframe[col].max())\n","        summary['Min'].append(dataframe[col].min())\n","        summary['Mean'].append(dataframe[col].mean())\n","        summary['Std'].append(dataframe[col].std())\n","        for num in [5, 25, 50, 75, 95]:\n","            summary[f\"{num}%\"].append(np.percentile(dataframe[col], num))\n","        summary['IQR'].append(summary['75%'][-1] - summary['25%'][-1])\n","        summary['Low Bound'].append(summary['50%'][-1]-1.5*summary['IQR'][-1])\n","        summary['High Bound'].append(summary['50%'][-1]+1.5*summary['IQR'][-1])\n","\n","        summary[\"< Low Bound\"].append(len(dataframe[dataframe[col]<summary[\"Low Bound\"][-1]]))\n","        summary[\"> High Bound\"].append(len(dataframe[dataframe[col]>summary[\"High Bound\"][-1]]))\n","        summary[\"Outliers\"].append(summary[\"< Low Bound\"][-1]+ summary[\"> High Bound\"][-1])\n","        summary[\"% Outliers\"].append(summary[\"Outliers\"][-1]/len(dataframe)*100.0)\n","    summary = pd.DataFrame(summary)\n","\n","    return summary\n","\n","def missing_values_table(dataframe):\n","    x = dataframe.isna().sum()/len(dataframe)*100\n","    x = x.reset_index()\n","    x.columns = ['feature', 'percentage-of-null']\n","    x = x.sort_values(by='percentage-of-null', ascending=False).reset_index(drop=True)\n","    x['unique'] = [dataframe[i].nunique() for i in x['feature']]\n","    x['unique-vals'] = [\",\".join (sorted(map(str,dataframe[i].unique()))) for i in x['feature']]\n","    # return x[x['percentage-of-null']>0]\n","    return x\n","\n","# Data Visualization and Analysis\n","# Numerical and Categorical visualization\n","def visualize_categorical_distributions(df):\n","    \"\"\"\n","    Visualizes the distribution of categorical features in the DataFrame.\n","\n","    Parameters:\n","    - df (pd.DataFrame): The DataFrame containing the categorical features to visualize.\n","\n","    Returns:\n","    - None: Displays the plots.\n","    \"\"\"\n","    # Identify categorical columns\n","    categorical_columns = df.select_dtypes(include=['object']).columns\n","\n","    # Set up the figure for multiple subplots\n","    num_cols = 3  # Number of columns for the subplot grid\n","    num_rows = (len(categorical_columns) + num_cols - 1) // num_cols  # Calculate number of rows needed\n","\n","    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\n","    fig.suptitle('Distribution of Categorical Features', fontsize=16)\n","\n","    # Flatten axes array for easy iteration\n","    axes = axes.flatten()\n","\n","    # Iterate over each categorical column and create a bar plot\n","    for i, col in enumerate(categorical_columns):\n","        sns.barplot(\n","            x=df[col].value_counts().index,\n","            y=df[col].value_counts().values,\n","            ax=axes[i],\n","            palette=\"viridis\"\n","        )\n","        # Setting titles and labels\n","        axes[i].set_title(f'Distribution of {col}', fontsize=14)\n","        axes[i].set_xlabel(col, fontsize=12)\n","        axes[i].set_ylabel('Count', fontsize=12)\n","        axes[i].tick_params(axis='x')\n","\n","    # Remove unused axes\n","    for j in range(i + 1, len(axes)):\n","        fig.delaxes(axes[j])\n","\n","    # Adjust layout\n","    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust the main title space\n","    plt.show()\n","\n","def visualize_numerical_distributions(df, exclude_columns='id'):\n","    \"\"\"\n","    Visualizes the distribution of numerical features in the DataFrame using histograms and KDE plots,\n","    excluding specified columns.\n","\n","    Parameters:\n","    - df (pd.DataFrame): The DataFrame containing the numerical features to visualize.\n","    - exclude_columns (list): List of column names to exclude from visualization.\n","\n","    Returns:\n","    - None: Displays the plots.\n","    \"\"\"\n","    # Ensure exclude_columns is a list\n","    if exclude_columns is None:\n","        exclude_columns = []\n","\n","    # Identify numerical columns and exclude specified ones\n","    numerical_columns = df.select_dtypes(include=['number']).columns\n","    numerical_columns = [col for col in numerical_columns if col not in exclude_columns]\n","\n","    # Set up the figure for multiple subplots\n","    num_cols = 3  # Number of columns for the subplot grid\n","    num_rows = (len(numerical_columns) + num_cols - 1) // num_cols  # Calculate number of rows needed\n","\n","    fig, axes = plt.subplots(num_rows, num_cols, figsize=(18, 5 * num_rows))\n","    fig.suptitle('Distribution of Numerical Features', fontsize=16)\n","\n","    # Flatten axes array for easy iteration\n","    axes = axes.flatten()\n","\n","    # Iterate over each numerical column and create a histogram with KDE\n","    for i, col in enumerate(numerical_columns):\n","        sns.histplot(df[col], kde=True, ax=axes[i], color=\"skyblue\", element=\"step\", stat=\"density\")\n","        axes[i].set_title(f'Distribution of {col}', fontsize=14)\n","        axes[i].set_xlabel(col, fontsize=12)\n","        axes[i].set_ylabel('Density', fontsize=12)\n","\n","    # Remove unused axes\n","    for j in range(i + 1, len(axes)):\n","        fig.delaxes(axes[j])\n","\n","    # Adjust layout\n","    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust the main title space\n","    plt.show()\n"],"metadata":{"id":"RjUCPrYYGMbq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Optional\n","def preprocess_categorical_data(df, df_t):\n","    \"\"\"\n","    Preprocesses the DataFrame by replacing non-alphabetical characters in categorical columns\n","    and imputing missing values in categorical columns using mode and in numeric columns using median.\n","\n","    Parameters:\n","    - df (pd.DataFrame): The input DataFrame.\n","\n","    Returns:\n","    - pd.DataFrame: The processed DataFrame.\n","    \"\"\"\n","    def replace_non_alphabetical(value):\n","        \"\"\"Replaces non-alphabetical values with NaN if they are not a single character.\"\"\"\n","        if isinstance(value, str) and (len(value) == 1 and value.isalpha()):\n","            return value\n","        return np.nan\n","\n","    # Identify categorical and numeric columns\n","    categorical_columns = df_t.select_dtypes(include=['object']).columns\n","    numeric_columns = df_t.select_dtypes(include=[np.number]).columns\n","\n","    # Replace non-alphabetical values with NaN in categorical columns\n","    for col in categorical_columns:\n","        df[col] = df[col].apply(replace_non_alphabetical)\n","        df_t[col] = df_t[col].apply(replace_non_alphabetical)\n","\n","    # Impute missing values in categorical columns using mode\n","    for col in categorical_columns:\n","        if df[col].isnull().any():\n","            mode_value = df[col].mode()[0] if not df[col].mode().empty else 'Unknown'  # Safe mode handling\n","            df[col].fillna(mode_value, inplace=True)\n","            df_t[col].fillna(mode_value, inplace=True)\n","\n","    # Impute missing values in numeric columns using median\n","    for col in numeric_columns:\n","        median_value = df[col].median()\n","        if df[col].isnull().any():\n","            df[col].fillna(median_value, inplace=True)\n","        df_t[col].fillna(median_value, inplace=True)\n","\n","    return df, df_t\n","\n","\n","# Apply the functions to train and test data\n","print(\"Imputing noise with mode for categoricals and median for numericals...\")\n","train, test = preprocess_categorical_data(df_train, df_test)\n","\n","\n","# Print summary of missing values after processing\n","print(\"\\nMissing values in train_data after processing: \", train.isnull().sum().sum())\n","print(\"\\nMissing values in test_data after processing: \", test.isnull().sum().sum())\n"],"metadata":{"id":"jAoXo-f1mo0Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def categorize_low_frequency_values(df_train, df_test, threshold=100):\n","    \"\"\"\n","    Replaces categories with less than a given threshold in all categorical columns\n","    with the category 'Other', applying the same transformation to both training and test datasets.\n","\n","    Parameters:\n","    - df_train (pd.DataFrame): The training DataFrame.\n","    - df_test (pd.DataFrame): The test DataFrame.\n","    - threshold (int): Frequency threshold below which categories are replaced with 'Other'.\n","\n","    Returns:\n","    - df_train (pd.DataFrame): Modified training DataFrame.\n","    - df_test (pd.DataFrame): Modified test DataFrame.\n","    - mapping_dict (dict): Dictionary containing the mappings of replaced values for each column.\n","    \"\"\"\n","    mapping_dict = {}\n","\n","    # Identify categorical columns\n","    categorical_columns = df_train.select_dtypes(include=['object']).columns\n","\n","    for col in categorical_columns:\n","        # Find values to replace\n","        value_counts = df_train[col].value_counts()\n","        values_to_replace = value_counts[value_counts < threshold].index\n","\n","        # Determine the mode of the column\n","        mode_value = df_train[col].mode()[0]\n","\n","        # Create mapping for the current column if there are values to replace\n","        if len(values_to_replace) > 0:\n","            # Store the mapping of original values to the mode\n","            mapping_dict[col] = {value: mode_value for value in values_to_replace}\n","\n","            # Replace in training and test data\n","            df_train[col] = df_train[col].replace(values_to_replace, mode_value)\n","            df_test[col] = df_test[col].replace(values_to_replace, mode_value)\n","\n","    return df_train, df_test, mapping_dict\n","\n","train, test, mappings = categorize_low_frequency_values(train, test)"],"metadata":{"id":"PM2Ngbhfm9-B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NgFq8mnNnn2B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scipy.stats import boxcox\n","def handle_skewness(df, df_t, threshold=1.0):\n","    \"\"\"\n","    Applies Box-Cox transformation to numerical columns in the DataFrame where skewness exceeds a threshold.\n","\n","    Parameters:\n","    - df (pd.DataFrame): The input DataFrame.\n","    - threshold (float): Skewness threshold to decide which columns to transform.\n","\n","    Returns:\n","    - pd.DataFrame: DataFrame with transformed columns.\n","    - dict: Dictionary of lambda values used for Box-Cox transformation for each column.\n","    \"\"\"\n","    numeric_cols = df.select_dtypes(include=['number']).columns\n","\n","    for col in numeric_cols:\n","        skewness = df[col].skew()\n","        # Check the skewness and ensure positive values for Box-Cox\n","        if skewness > threshold:\n","            # Adding 1 to shift all data to positive if there are zero or negative values\n","            df[col] = df[col] + 1\n","            df[col], fitted_lambda = boxcox(df[col])\n","            df_t[col] = df_t[col] + 1\n","            df_t[col] = boxcox(df_t[col], lmbda=fitted_lambda)\n","\n","\n","    return df, df_t\n","\n","# Example usage:\n","# df is your DataFrame containing the numerical data\n","train, test = handle_skewness(train, test)\n"],"metadata":{"id":"O8FeOmeOnhjo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def visualize_box_plots(df):\n","    \"\"\"\n","    Visualizes the distribution of numerical features in the DataFrame using box plots to identify outliers.\n","\n","    Parameters:\n","    - df (pd.DataFrame): The DataFrame containing the numerical features to visualize.\n","\n","    Returns:\n","    - None: Displays the box plots.\n","    \"\"\"\n","    # Identify numerical columns\n","    numerical_columns = df.select_dtypes(include=['number']).columns\n","\n","    # Set up the figure for multiple subplots\n","    num_cols = 3  # Number of columns for the subplot grid\n","    num_rows = (len(numerical_columns) + num_cols - 1) // num_cols  # Calculate number of rows needed\n","\n","    fig, axes = plt.subplots(num_rows, num_cols, figsize=(18, 5 * num_rows))\n","    fig.suptitle('Box Plot of Numerical Features', fontsize=16)\n","\n","    # Flatten axes array for easy iteration\n","    axes = axes.flatten()\n","\n","    # Iterate over each numerical column and create a box plot\n","    for i, col in enumerate(numerical_columns):\n","        sns.boxplot(x=df[col], ax=axes[i], color=\"skyblue\")\n","        axes[i].set_title(f'Box Plot of {col}', fontsize=14)\n","        axes[i].set_xlabel(col, fontsize=12)\n","\n","    # Remove unused axes\n","    for j in range(i + 1, len(axes)):\n","        fig.delaxes(axes[j])\n","\n","    # Adjust layout\n","    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust the main title space\n","    plt.show()\n","\n","visualize_box_plots(train)"],"metadata":{"id":"-B3NnP2QnpRJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_outliers_percentage(df):\n","    \"\"\"\n","    Calculates the percentage of data considered outliers based on the IQR method for each numerical column.\n","\n","    Parameters:\n","    - df (pd.DataFrame): DataFrame to analyze.\n","\n","    Returns:\n","    - None: Prints the percentage of outliers for each numerical column.\n","    \"\"\"\n","    outlier_counts = {}\n","    for column in df.select_dtypes(include=['number']).columns:\n","        Q1 = df[column].quantile(0.25)\n","        Q3 = df[column].quantile(0.75)\n","        IQR = Q3 - Q1\n","        lower_bound = Q1 - 1.5 * IQR\n","        upper_bound = Q3 + 1.5 * IQR\n","\n","        # Calculate outliers\n","        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n","        outlier_counts[column] = len(outliers)\n","\n","    # Print the percentage of outliers for each column\n","    for column in outlier_counts:\n","        percentage = (outlier_counts[column] / len(df)) * 100\n","        print(f\"Percentage of outliers in {column}: {percentage:.2f}%\")\n","\n","# Example usage:\n","calculate_outliers_percentage(train)"],"metadata":{"id":"0kEFkQYVmfAH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def handle_outliers(df,df_t):\n","    \"\"\"\n","    Handles outliers in a DataFrame by capping based on the IQR method.\n","\n","    Parameters:\n","    - df (pd.DataFrame): DataFrame to process.\n","\n","    Returns:\n","    - pd.DataFrame: DataFrame with outliers handled.\n","    \"\"\"\n","    for column in ['cap-diameter', 'stem-height', 'stem-width']:\n","        Q1 = df[column].quantile(0.25)\n","        Q3 = df[column].quantile(0.75)\n","        IQR = Q3 - Q1\n","        lower_bound = Q1 - 1.5 * IQR\n","        upper_bound = Q3 + 1.5 * IQR\n","\n","        # Capping\n","        df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)\n","        df_t[column] = df_t[column].clip(lower=lower_bound, upper=upper_bound)\n","\n","    return df, df_t\n","\n","# Apply to both training and test datasets\n","train, test = handle_outliers(train, test)"],"metadata":{"id":"Jb_Mq9e_n8tZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.to_csv('train_processed.csv', index=False)\n","test.to_csv('test_processed.csv', index=False)"],"metadata":{"id":"maQSYlSmn_qB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with mlflow.start_run() as run:\n","    artifact_uri, run_id = run.info.artifact_uri, run.info.run_id\n","    mlflow.log_artifact(\"train_processed.csv\")\n","    mlflow.log_artifact(\"test_processed.csv\")"],"metadata":{"id":"KxK9fuBWoGiw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# _Plotting and Analysis (Common)"],"metadata":{"id":"UJM2Upv4LGzW"}},{"cell_type":"code","source":["num_rows = (len(num_features)+1)//2\n","fig = make_subplots(rows=num_rows, cols=2)\n","for i in range(num_rows):\n","    # fig.add_trace(px.box(df_1,y=numerical_cols[i]),row=i//2+1,col=i%2+1)\n","    fig.add_trace(go.Box(y=df_train[num_features[i]], name=num_features[i]),row=i//2+1,col=i%2+1)\n","\n","fig.update_layout(title_text='Box plot of Numerical columns',height=800, width=1100)\n","fig.show()"],"metadata":{"id":"gCEGDyfeePtv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i, df in enumerate([df_train, df_extra, df_test]):\n","    # If target_feature is  categorical, you will nedd to change the following code\n","    try:\n","        corr = df[num_features+[target_feature,]].corr()\n","    except KeyError as e:\n","        corr = df[num_features].corr()\n","    # Generate a mask for the upper triangle\n","    mask = np.triu(np.ones_like(corr, dtype=bool))\n","\n","    # Set up the matplotlib figure\n","    f, ax = plt.subplots(figsize=(11, 9))\n","\n","    # Generate a custom diverging colormap\n","    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n","\n","    # Draw the heatmap with the mask and correct aspect ratio\n","    sns.heatmap(corr, mask=mask, cmap='coolwarm', vmax=1, center=0,\n","                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n","\n","    plt.title('Correlation Matrix', fontsize=20)\n","    !mkdir -p output/images\n","    plt.savefig(f\"output/images/Correlation matrix_{i+1}.png\")\n","    plt.show()"],"metadata":{"id":"ejq_lSp5YkzB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### $\\chi^2$ for categorical features vs target feature\n","# $H_0$ : The categorical feature and taget feature are distributed independently.\n","\n","df = pd.concat([df_train, df_extra]).reset_index(drop=True)\n","\n","from scipy.stats import chi2_contingency\n","temp = pd.DataFrame([], columns = ['Feature', 'Test Statistic', 'P Value', 'Rejected'])\n","threshold = 0.05\n","for i, col in enumerate(cat_features):\n","    res = chi2_contingency(pd.crosstab(df[col], df[target_feature]))\n","    temp.loc[i] = [col, round(res.statistic,3), round(res.pvalue,3), res.pvalue<threshold]\n","temp"],"metadata":{"id":"uWnt8dCkoSLx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","**ANOVA test**\n","\n","$H_0$ : The population mean of all of the groups are equal.\n","\n","**The Kruskal-Wallis H-test**\n","\n","$H_0$ : The population median of all of the groups are equal.\n","\"\"\"\n","from scipy.stats import f_oneway, kruskal\n","temp = pd.DataFrame()\n","temp['Feature'] = num_features\n","groups = []\n","for i in df[target_feature].unique():\n","    groups.append(df[df[target_feature]==i][num_features])\n","F, p = f_oneway(groups[0], groups[1], groups[2], groups[3], groups[4], groups[5], groups[6] )\n","temp['Anova_stats'] = np.round(F,3)\n","temp['Anova_pvalue'] = np.round(p,3)\n","F, p = kruskal(groups[0], groups[1], groups[2], groups[3], groups[4], groups[5], groups[6] )\n","temp['Krushkal_stats'] = np.round(F,3)\n","temp['Krushkal_pvalue'] = np.round(p,3)\n","temp"],"metadata":{"id":"yYEzfCZJmwF1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RX5jYDU5mbTg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# _Optimisation"],"metadata":{"id":"6nyBadB3EKdq"}},{"cell_type":"code","source":["res_ = {\n","    'trial_id'  : [],\n","    'accuracy'  : [],\n","    'margin'    : [],\n","    'params'    : [],\n","}\n","for trial in study.best_trials:\n","    res_['trial_id'].append(trial.number)\n","    res_['params'].append(trial.params)\n","    res_['accuracy'].append(trial.values[0])\n","    res_['margin'].append(trial.values[1])\n","\n","result_df = pd.DataFrame(res_)\n","select_cols = ['trial_id', 'accuracy', 'test_acc', 'margin', 'params']\n","result_df['test_acc'] = result_df['accuracy'] + result_df['margin']\n","result_df = result_df[select_cols]\n","\n","result_df.sort_values(['accuracy','margin'], ascending=[False, False]).reset_index(drop=True)\n","result_df.sort_values(['test_acc','margin'], ascending=[False, False]).reset_index(drop=True)\n","result_df.sort_values(['margin', 'test_acc',], ascending=[False, False]).reset_index(drop=True)"],"metadata":{"id":"F5LiCpu2EQXy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Gh8JyzThlB0K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RrU7G8mclBsv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jZ0W5u6-lBk3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yE26D9-DlBVx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# _Saving your data"],"metadata":{"id":"3FUt4np_jw3d"}},{"cell_type":"code","source":["description = \" \"\n","with mlflow.start_run(description= description):\n","    mlflow.log_artifact('output')"],"metadata":{"id":"32IN7xITj1CJ"},"execution_count":null,"outputs":[]}]}